Eurostat Lakehouse on AWS

Bronze â†’ Silver â†’ Gold | Airflow Orchestration | S3 Publishing | Production-Style Data Engineering

Overview

This repository implements a production-style Data Engineering project using Eurostat Structural Business Statistics (SBS) datasets.

The pipeline follows a Lakehouse architecture pattern (Bronze â†’ Silver â†’ Gold), orchestrated with Apache Airflow locally, and publishes curated outputs to AWS S3.

This project demonstrates:

End-to-end data pipeline design

Layered lakehouse modeling

Airflow DAG orchestration

Data quality validation

AWS S3 integration

Reproducible local-to-cloud workflow

Git-based version control

The goal is to simulate a real-world analytics engineering workflow using public economic data.

Architecture
High-Level Flow
Eurostat TSV (Raw)
        â†“
ğŸ¥‰ Bronze Layer (Parquet, minimal transformation)
        â†“
ğŸ¥ˆ Silver Layer (Normalized + Cleaned + Typed)
        â†“
ğŸ¥‡ Gold Layer (Analytical marts + KPIs)
        â†“
ğŸ“Š HTML Analytical Report
        â†“
â˜ AWS S3 (Curated publishing)

Lakehouse Layers
ğŸ¥‰ Bronze Layer

Reads Eurostat TSV exactly as downloaded

Preserves raw structure

Adds ingestion metadata:

source_file

ingested_at

Converts to Parquet

Stored in:

data-bronze/


Minimal transformation, schema preserved.

ğŸ¥ˆ Silver Layer

Splits Eurostat composite dimension column:

freq,nace_r2,indic_sbs,geo


Converts wide year columns:

2005, 2006, 2007 ...


â†’ into normalized format:

year | value_raw


Cleans Eurostat flags:

:

e

b

p

Casts numeric fields

Produces consistent schema

Stored in:

data-silver/

ğŸ¥‡ Gold Layer

Analytical marts designed for reporting and BI consumption.

Includes:

gold_country_indicator_year.parquet

gold_structural_metrics.parquet

gold_yoy_growth.parquet

Metrics include:

Country rankings

Year-over-Year growth

CAGR

Top movers

Market leaders by indicator

Stored in:

data-gold/

Airflow Orchestration

The pipeline is orchestrated through a DAG:

airflow/dags/eurostat_lakehouse_dag.py


Task flow:

download_raw
    â†“
bronze_ingest
    â†“
silver_transform
    â†“
gold_analytics
    â†“
quality_checks
    â†“
generate_html_report


The DAG ensures:

Dependency control

Reproducibility

Modular execution

Clear separation of concerns

HTML Analytical Report

After Gold layer generation, the pipeline produces:

reports/out/gold_report.html


This report includes:

Executive snapshot

Top 10 countries

YoY growth charts

Rank movers

CAGR performance

Structural business metrics

The report simulates a business-ready analytics deliverable.

AWS Integration (S3 Publishing)

This project publishes curated outputs to AWS S3.

Example structure inside S3:

s3://mauricio-eurostat-lakehouse-prod/
    â”œâ”€â”€ bronze/
    â”œâ”€â”€ silver/
    â”œâ”€â”€ gold/
    â””â”€â”€ reports/
        â”œâ”€â”€ gold_report.html
        â””â”€â”€ assets/


Publishing is done via AWS CLI:

aws s3 cp reports/out/gold_report.html s3://bucket-name/reports/gold_report.html
aws s3 sync reports/out/assets s3://bucket-name/reports/assets


This demonstrates:

Cloud integration

Storage layer separation

Production-style artifact publishing

Lakehouse to object storage workflow

Repository Structure
eurostat-lakehouse/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ data-raw/
â”œâ”€â”€ data-bronze/
â”œâ”€â”€ data-silver/
â”œâ”€â”€ data-gold/
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ out/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 00_download_raw.py
â”‚   â”œâ”€â”€ 01_extract_raw.py
â”‚   â”œâ”€â”€ 02_bronze_ingest.py
â”‚   â”œâ”€â”€ 03_silver_transform.py
â”‚   â”œâ”€â”€ 04_gold_analytics.py
â”‚   â”œâ”€â”€ 05_quality_checks.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

How to Run (Local)
1. Create Virtual Environment

Windows PowerShell:

python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
pip install -r requirements.txt

2. Run Pipeline (Manual)
python src/00_download_raw.py
python src/02_bronze_ingest.py
python src/03_silver_transform.py
python src/04_gold_analytics.py
python src/05_quality_checks.py

3. Run via Airflow (Docker)

Inside airflow/:

docker compose up -d


Open:

http://localhost:8080


Enable:

eurostat_lakehouse_dag

Data Quality Checks

Includes:

Row count validation per layer

Schema validation

Null rate checks

Numeric conversion checks

Layer consistency validation

Outputs saved in:

outputs-checks/

Technical Skills Demonstrated

Lakehouse modeling

Data normalization

ETL modularization

Airflow orchestration

Data quality engineering

Cloud storage integration (AWS S3)

CLI automation

Analytical data mart design

Reproducible local development workflow

Git versioning

Data Source

Eurostat â€“ Structural Business Statistics (SBS)
https://ec.europa.eu/eurostat

Author

Mauricio Esquivel
Data Engineer | Analytics Engineer
Focus: Lakehouse Architectures, Airflow, AWS, Databricks-style pipelines, Cloud Analytics
