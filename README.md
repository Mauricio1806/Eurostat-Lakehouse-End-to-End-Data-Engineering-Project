ğŸ‡ªğŸ‡º Eurostat Lakehouse on AWS
Bronze â†’ Silver â†’ Gold Â· Airflow Orchestration Â· AWS S3 Publishing Â· Production-Style Data Engineering
ğŸ“Œ Overview

This project implements a production-style end-to-end Data Engineering pipeline using Eurostat Structural Business Statistics (SBS).

It follows a Lakehouse architecture pattern (Bronze â†’ Silver â†’ Gold), orchestrated with Apache Airflow and integrated with AWS S3 for cloud publishing.

The objective is to simulate a real-world analytics engineering workflow using public European economic data.

ğŸ¯ What This Project Demonstrates

Layered Lakehouse modeling

ETL modular design

Airflow DAG orchestration

Data normalization & cleaning

Analytical mart construction

HTML report generation

AWS S3 cloud publishing

CLI automation

Reproducible local workflow

Git-based versioning

ğŸ— Architecture
High-Level Data Flow
Eurostat TSV (Raw)
        â†“
ğŸ¥‰ Bronze (Parquet - raw preserved + metadata)
        â†“
ğŸ¥ˆ Silver (Normalized + Cleaned + Typed)
        â†“
ğŸ¥‡ Gold (Analytical Marts)
        â†“
ğŸ“Š HTML Business Report
        â†“
â˜ AWS S3 (Curated Publishing Layer)

ğŸ§± Lakehouse Layers
ğŸ¥‰ Bronze Layer

Purpose: Preserve raw data with minimal transformation.

Reads TSV exactly as downloaded

Adds ingestion metadata:

source_file

ingested_at

Converts to Parquet

Schema preserved

Stored in:

data-bronze/

ğŸ¥ˆ Silver Layer

Purpose: Clean and normalize data for analytical readiness.

Transformations:

Split composite dimension column:

freq,nace_r2,indic_sbs,geo


Convert wide year columns:

2005, 2006, 2007 ...


â†’ to normalized format:

year | value_raw


Clean Eurostat flags:

:

e

b

p

Cast numeric fields

Standardize schema

Stored in:

data-silver/

ğŸ¥‡ Gold Layer

Purpose: Create analytical marts for reporting & BI.

Includes:

Country rankings

Year-over-Year growth

CAGR

Top movers

Indicator-level aggregation

Main outputs:

gold_country_indicator_year.parquet
gold_structural_metrics.parquet
gold_yoy_growth.parquet


Stored in:

data-gold/

ğŸ” Airflow Orchestration

DAG:

airflow/dags/eurostat_lakehouse_dag.py


Pipeline execution order:

download_raw
    â†“
bronze_ingest
    â†“
silver_transform
    â†“
gold_analytics
    â†“
quality_checks
    â†“
generate_html_report


Ensures:

Controlled task dependencies

Reproducibility

Clear modular separation

Production-style orchestration

ğŸ“Š HTML Analytical Report

After Gold layer generation:

reports/out/gold_report.html


Includes:

Executive summary

Market leaders

YoY growth charts

Rank movers

CAGR performance

Structural business metrics

This simulates a business-facing analytics deliverable.

â˜ AWS Integration (S3 Publishing)

The project publishes curated artifacts to AWS S3.

Example structure:

s3://mauricio-eurostat-lakehouse-prod/
â”‚
â”œâ”€â”€ bronze/
â”œâ”€â”€ silver/
â”œâ”€â”€ gold/
â””â”€â”€ reports/
    â”œâ”€â”€ gold_report.html
    â””â”€â”€ assets/


Publishing commands:

aws s3 cp reports/out/gold_report.html \
    s3://mauricio-eurostat-lakehouse-prod/reports/gold_report.html

aws s3 sync reports/out/assets \
    s3://mauricio-eurostat-lakehouse-prod/reports/assets


This demonstrates:

Object storage integration

Curated artifact publishing

Lakehouse-to-cloud workflow

Cloud-ready architecture

ğŸ“‚ Repository Structure
eurostat-lakehouse/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ data-raw/
â”œâ”€â”€ data-bronze/
â”œâ”€â”€ data-silver/
â”œâ”€â”€ data-gold/
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ out/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 00_download_raw.py
â”‚   â”œâ”€â”€ 01_extract_raw.py
â”‚   â”œâ”€â”€ 02_bronze_ingest.py
â”‚   â”œâ”€â”€ 03_silver_transform.py
â”‚   â”œâ”€â”€ 04_gold_analytics.py
â”‚   â”œâ”€â”€ 05_quality_checks.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸš€ Running Locally
1ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
pip install -r requirements.txt

2ï¸âƒ£ Run Pipeline (Manual)
python src/00_download_raw.py
python src/02_bronze_ingest.py
python src/03_silver_transform.py
python src/04_gold_analytics.py
python src/05_quality_checks.py

3ï¸âƒ£ Run with Airflow (Docker)
cd airflow
docker compose up -d


Open:

http://localhost:8080


Enable:

eurostat_lakehouse_dag

âœ… Data Quality Checks

The pipeline validates:

Row count consistency

Schema validation

Null rate analysis

Numeric conversion success

Layer consistency

Outputs stored in:

outputs-checks/

ğŸ›  Technical Stack

Python

Pandas

PyArrow

Apache Airflow

Docker

AWS CLI

Amazon S3

HTML reporting

Lakehouse modeling pattern

ğŸ“š Data Source

Eurostat â€“ Structural Business Statistics (SBS)
https://ec.europa.eu/eurostat

ğŸ‘¤ Author

Mauricio Esquivel
Data Engineer | Analytics Engineer

Focus Areas:

Lakehouse Architectures

Airflow Orchestration

AWS & Cloud Storage

Analytics Engineering

Reproducible Data Pipelines
